import requests
import re
from bs4 import BeautifulSoup
import csv
from datetime import datetime

# Searching for:
# number of words
# Writing text in console and to CSV file


def CSV_file(search_final, write, url, write_old):
    global dubliojantis_textas
    dubliojantis_textas = 0
    for dublicate in write_old:
        if dublicate == write:
            dubliojantis_textas = 1
    if (dubliojantis_textas != 1):
        write_old.append(write)
        if copy_text != write:
            print(f'{datetime.now()}___"{search_final}"')
            print(url)
            print(write)
            print("\n")
            with open(f"{language}_{search_final}_{datetime.date(datetime.now())}.csv", "a", encoding="UTF-8",
                      newline='') as file:
                csv_writer = csv.writer(file)
                csv_writer.writerow([search_final, write, url])
            copy_txt = write

# Dinding words in the text
def filtre(search_for, search_for2, line, url, distance):
    for search in search_for:
        if line.find(search) != -1:
            for search2 in search_for2:
                if (line.find(search2) != -1) and (abs(line.find(search2) - line.find(search)) < (nr_word/distance)):
                    search_final = search.upper() + " " + search2.upper()
                    write = line[line.find(search):line.find(search2) + nr_word]
                else:
                    write = line[line.find(search):line.find(search) + nr_word]
                    search_final = search.upper()
                CSV_file(search_final, write, url, write_old)

# Reading the text from html
def reading_text(url):
    # print(url)
    res = requests.get(url)
    html_page = res.content
    soup = BeautifulSoup(html_page, 'html.parser')
    # print(soup)
    text = soup.find_all(search_symbol)
    # print(text)
    output = ''
    for t in text:
        get_text = t.get_text()
        output += get_text
    filtre(search_for, search_for2, output, url, distance)


def remove_duplicates(l,links):  # remove duplicates and unURL string
    for item in l:
        match = re.search("(?P<url>https?://[^\s]+)", item)
        if match is not None:
            links.append((match.group("url")))
    # return links

def web_links_collector(all_path):
    for path in all_path:
        try:
            # collecting url in the web page as a list of the url
            print(path)
            source_code = requests.get(path)
            soup = BeautifulSoup(source_code.content, 'html.parser')
            # print(soup)
            data = []
            links = []

            for link in soup.find_all('a', href=True):
                data.append(str(link.get('href')))
            remove_duplicates(data, links)
            for url in links:
                # print(url)
                reading_text(url)
        except:
            print("This page is not reachable")
            continue


# Searching for words

global search_for, search_for2, search_for3, nr_word, copy_text, language, search_symbol, write_old, distance

copy_text = ""
write_old = [""]
distance = 50


# Lithuanian

language = "LT"
search_for = ["Naftos ", "Duj≈≥ ", "Elektros"]

search_for2 = ["kain", "versl", "rink"]

all_path = ['https://www.vz.lt/', 'https://www.delfi.lt/']
nr_word = 30
print(datetime.now())
print("Looking for lithuanian text in Lithuanian Web pages")
search_symbol = 'a'
for url in all_path:
    print(url)
    reading_text(url)
nr_word = 130
print(datetime.now())
print("Looking for lithuanian text in Lithuanian Web pages and all links")
search_symbol = 'p'
web_links_collector(all_path)


# English

language = "EN"
search_for = ["oil", "LNG", "Energy"]

search_for2 = ["business", "price ", "market"]

all_path = ['https://www.ft.com/', 'https://www.bbc.com/']

nr_word = 30
print(datetime.now())
print("Looking for lithuanian text in Lithuanian Web pages")
search_symbol = 'a'
for url in all_path:
    print(url)
    reading_text(url)

nr_word = 130
print(datetime.now())
print("Looking for lithuanian text in Lithuanian Web pages and all links")
search_symbol = 'p'
web_links_collector(all_path)
