import requests
import re
from bs4 import BeautifulSoup
import csv
from datetime import datetime


# Searching for:
# number of words
# Writing text in to CSV file
def CSV_file(search_final, url, write):
    if copy_text != write:
        with open(f"{datetime.date(datetime.now())}_{search_final}_{language}.csv", "a", encoding="UTF-8",
                  newline='') as file:
            csv_writer = csv.writer(file)
            csv_writer.writerow([search_final, url, write])
        copy_txt = write


def filtre(search_for, search_for2, line, url):
    for search in search_for:
        if line.find(search) != -1:
            print(url)
            print(f'{datetime.now()} {search.upper()} has been found in the text!')
            print('----------------------------------------------------------------------------------------')
            write = line[line.find(search):line.find(search) + nr_word]
            print(write)
            search_final = search.upper()
            # Recording in CSV file
            CSV_file(search_final, url, write)

            for search2 in search_for2:
                if (line.find(search2) != -1) and (abs(line.find(search2) - line.find(search)) < (nr_word * 3)):

                    print(url)
                    print(f'{datetime.now()} {search.upper()} + {search2.upper()} has been found in the text!')
                    print('----------------------------------------------------------------------------------------')

                    if line.find(search) > line.find(search2):
                        write0 = line[line.find(search2) - nr_word:line.find(search) + nr_word]
                        # Finding if there are many space
                        count = 0
                        for i in write0:
                            if (i.isspace()):
                                count = count + 1
                            else:
                                count = count - 1
                        if count < 0:
                            write = write0
                            print(write)
                            search_final = search.upper() + " " + search2.upper()
                            # Recording in CSV file
                            CSV_file(search_final, url, write)
                    else:
                        write0 = line[line.find(search) - nr_word:line.find(search2) + nr_word]
                        # Finding if there are many space
                        count = 0
                        for i in write0:
                            if (i.isspace()):
                                count = count + 1
                            else:
                                count = count - 1
                        if count < 0:
                            write = write0
                            print(write)

                            search_final = search.upper() + " " + search2.upper()
                            # Recording in CSV file
                            CSV_file(search_final, url, write)


# Text reader
def reading_text(url):
    # print(url)
    res = requests.get(url)
    html_page = res.content
    soup = BeautifulSoup(html_page, 'html.parser')
    text = soup.find_all(search_symbol)
    output = ''
    for t in text:
        get_text = t.get_text()
        output += get_text
    filtre(search_for, search_for2, output, url)


def remove_duplicates(l, links):  # remove duplicates and unURL string
    for item in l:
        match = re.search("(?P<url>https?://[^\s]+)", item)
        if match is not None:
            links.append((match.group("url")))


def web_links_collector(all_path):
    for path in all_path:
        # collecting url in the web page as a list of the url
        source_code = requests.get(path)
        soup = BeautifulSoup(source_code.content, 'html.parser')
        data = []
        links = []
        for link in soup.find_all('a', href=True):
            data.append(str(link.get('href')))
        flag = True
        remove_duplicates(data, links)
        while flag:
            try:
                for link in links:
                    for j in soup.find_all('a', href=True):
                        temp = []
                        source_code = requests.get(link)
                        soup = BeautifulSoup(source_code.content, 'html.parser')
                        temp.append(str(j.get('href')))
                        remove_duplicates(temp, link)
                        if len(links) > 162:  # set limitation to number of URLs
                            break
                    if len(links) > 162:
                        break
                if len(links) > 162:
                    break
            except Exception as e:
                print(e)
                if len(links) > 162:
                    break
        List_url = []
        for url in links:
            List_url.append(url)
            # print(url)
            reading_text(url)


# Searching for words
global search_for, search_for2, search_for3, nr_word, copy_text, language, search_symbol

copy_text = ""

# Lithuanian

language = "Lietuviškai"
search_for = ["LG Keleiviams", "Lietuvos geležinkel", "Rail Baltica", "Orlen Lietuv", "Klaipėdos naft",
              "Klaipėdos vand", "Klaipėdos uost", "Dujų terminal", "Leituvos energetik",
              "Transporto sektor", "Energetikos sektor", "Dirbtinis intelekt", "IT sektor", "naftos"]

search_for2 = ["nuostol", "peln", "akcijos", "gerėj", "blogėj", "kain"]

all_path = ['https://www.vz.lt/', 'https://www.15min.lt/', 'https://www.delfi.lt/']
nr_word = 100
print(datetime.now())
print("Looking for lithuanian text in Lithuanian Web pages")
search_symbol = 'a'
for url in all_path:
    # print(url)
    reading_text(url)
nr_word = 150
print(datetime.now())
print("Looking for lithuanian text in Lithuanian Web pages and all links")
search_symbol = 'p'
web_links_collector(all_path)

