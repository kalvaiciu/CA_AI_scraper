from glob import glob
from typing import BinaryIO
import requests
import re
import time
from bs4 import BeautifulSoup
from wsgiref import headers
import csv
import timeit
from datetime import datetime



# Searching for:
# number of words
# Į CSV faila įrašymas
def CSV_file(search, url, write):
    if copy_text != write:
        with open(f"{search}.csv", "a", encoding="UTF-8", newline='') as file:
            csv_writer = csv.writer(file)
            # csv_writer.writerow(['Search', 'url', 'Text'])
            csv_writer.writerow([search, url, write])
        copy_txt = write
def filtre(search_for, line, url):
    for search in search_for:
        if line.find(search) != -1:
            print(url)
            print(search)
            write = line[line.find(search):line.find(search) + nr_word]
            print(write)
            print("-------------------------------------------------------------------------------------------------")
            CSV_file(search, url, write)


# Pasirinkto puslapio nuskaitymas
def reading_text(url, links):
    # print(url)
    res = requests.get(url)
    html_page = res.content
    soup = BeautifulSoup(html_page, 'html.parser')
    text = soup.find_all('p')
    output = ''
    for t in text:
        get_text = t.get_text()
        output += get_text
    filtre(search_for, output, url)
    
def remove_duplicates(l,links):  # remove duplicates and unURL string
    for item in l:
        match = re.search("(?P<url>https?://[^\s]+)", item)
        if match is not None:
            links.append((match.group("url")))

def web_links_collector(all_path):
    for path in all_path:
        # collecting url in the web page as a list of the url
        source_code = requests.get(path)
        soup = BeautifulSoup(source_code.content, 'lxml')
        data = []
        links = []
        for link in soup.find_all('a', href=True):
            data.append(str(link.get('href')))
        flag = True
        remove_duplicates(data,links)
        while flag:
            try:
                for link in links:
                    for j in soup.find_all('a', href=True):
                        temp = []
                        source_code = requests.get(link)
                        soup = BeautifulSoup(source_code.content, 'lxml')
                        temp.append(str(j.get('href')))
                        remove_duplicates(temp,link)
                        if len(links) > 162:  # set limitation to number of URLs
                            break
                    if len(links) > 162:
                        break
                if len(links) > 162:
                    break
            except Exception as e:
                print(e)
                if len(links) > 162:
                    break
        List_url = []
        for url in links:
            List_url.append(url)
            # print(url)
            start = datetime.now()
            print(url)
            print(f"{start}, starting")
            reading_text(url, links)
            stop = datetime.now()
            print(f"{stop}, stopped")
            time_dif = stop - start
            print(f"{time_dif}, time dif")

# Searching for words
search_for = ["LG Keleiviams", "Lietuvos geležinkel", "Rail Baltica", "Orlen Lietuv", "Klaipėdos naft",
              "Klaipėdos vand",
              "Klaipėdos uost", "Dujų terminal", "Leituvos energetik", "Covid-19 virus",
              "Transporto sektor", "Energetikos sektor", "Dirbtinis intelekt", "IT sektor"]
nr_word = 200
copy_text = ""

all_path = ['https://www.15min.lt/', 'https://www.delfi.lt/', 'https://www.tv3.lt/']
start_with_main_page = input("Do You want to find the information from the main web pages? if Yes enter Y:    ")
if start_with_main_page.upper() == "Y":
    for url in all_path:
        start = datetime.now()
        print(url)
        print(f"{start}, starting")
        reading_text(url, all_path)
        stop = datetime.now()
        print(f"{stop}, stopped")
        time_dif = stop - start
        print(f"{time_dif}, time dif")
start_with_main_page_and_links_inside = input("Do You want to find the information from the main web pages and all the links inside? if Yes enter Y:    ")
if start_with_main_page_and_links_inside.upper() == "Y":
    web_links_collector(all_path)
